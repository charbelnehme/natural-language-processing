{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 12 - Tales from the Crypto\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sentiment Analysis\n",
    "\n",
    "Use the [newsapi](https://newsapi.org/) to pull the latest news articles for Bitcoin and Ethereum and create a DataFrame of sentiment scores for each coin.\n",
    "\n",
    "Use descriptive statistics to answer the following questions:\n",
    "1. Which coin had the highest mean positive score?\n",
    "2. Which coin had the highest negative score?\n",
    "3. Which coin had the highest positive score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP Homework Assignment\n",
      "Student Name: Charbel Nehme\n"
     ]
    }
   ],
   "source": [
    "# Initial imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import alpaca_trade_api as tradeapi\n",
    "import nltk as nltk\n",
    "from newsapi.newsapi_client import NewsApiClient\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "import matplotlib.pyplot as plt\n",
    "import dataframe_image as dfi\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print('NLP Homework Assignment')\n",
    "print('Student Name: Charbel Nehme')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env enviroment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set News API Key\n",
    "newsapi = NewsApiClient(api_key=os.environ[\"NEWS_API_KEY\"])\n",
    "\n",
    "# Set Alpaca API key and secret\n",
    "alpaca_api_key = os.getenv(\"ALPACA_API_KEY\")\n",
    "alpaca_secret_key = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "\n",
    "api = tradeapi.REST(alpaca_api_key, alpaca_secret_key, api_version='v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a newsapi client\n",
    "api_key = os.getenv(\"NEWS_API_KEY\")\n",
    "newsapi = NewsApiClient(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are {\"totalResults\"} Bitcoin news articles in the English media.\n"
     ]
    }
   ],
   "source": [
    "# Fetch the Bitcoin news articles\n",
    "bitcoin_news_en = newsapi.get_everything(\n",
    "    q=\"bitcoin\",\n",
    "    language=\"en\"\n",
    ")\n",
    "\n",
    "# Show the total number of news\n",
    "bitcoin_news_en[\"totalResults\"]\n",
    "print('There are {\"totalResults\"} Bitcoin news articles in the English media.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are {\"totalResults\"} Ethereum news articles in the English media\n"
     ]
    }
   ],
   "source": [
    "# Fetch the Ethereum news articles\n",
    "ethereum_news_en = newsapi.get_everything(\n",
    "    q=\"bitcoin\",\n",
    "    language=\"en\"\n",
    ")\n",
    "\n",
    "# Show the total number of news\n",
    "ethereum_news_en[\"totalResults\"]\n",
    "print('There are {\"totalResults\"} Ethereum news articles in the English media')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a dataframe for Bitcoin news in English\n",
    "def create_df(news, language):\n",
    "    articles = []\n",
    "    for article in news:\n",
    "        try:\n",
    "            title = article[\"title\"]\n",
    "            description = article[\"description\"]\n",
    "            text = article[\"content\"]\n",
    "            date = article[\"publishedAt\"][:10]\n",
    "\n",
    "            articles.append({\n",
    "                \"title\": title,\n",
    "                \"description\": description,\n",
    "                \"text\": text,\n",
    "                \"date\": date,\n",
    "                \"language\": language\n",
    "            })\n",
    "        except AttributeError as ae:\n",
    "            pass\n",
    "\n",
    "    return pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ex-Party Producer Charged in $2.7 Million Bitc...</td>\n",
       "      <td>Thomas Spieker’s clients included dark-web dru...</td>\n",
       "      <td>Mr. Spiekers most prominent customer, whom he ...</td>\n",
       "      <td>2022-03-24</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Everyone Has Crypto FOMO, but Does It Belong i...</td>\n",
       "      <td>A growing array of investment options make it ...</td>\n",
       "      <td>Other fund vehicles hold crypto directly, but ...</td>\n",
       "      <td>2022-03-24</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you’re a Russian YouTuber, how do you get p...</td>\n",
       "      <td>Russian creators are shut off from the global ...</td>\n",
       "      <td>When Russia invaded Ukraine, Niki Proshin was ...</td>\n",
       "      <td>2022-03-17</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El Salvador turns to Binance for help on bitco...</td>\n",
       "      <td>El Salvador is seeking support from cryptocurr...</td>\n",
       "      <td>SAN SALVADOR, March 23 (Reuters) - El Salvador...</td>\n",
       "      <td>2022-03-23</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why Isn't Bitcoin Booming?</td>\n",
       "      <td>\"Bitcoin was seen by many of its libertarian-l...</td>\n",
       "      <td>\"Bitcoin was seen by many of its libertarian-l...</td>\n",
       "      <td>2022-03-12</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Ex-Party Producer Charged in $2.7 Million Bitc...   \n",
       "1  Everyone Has Crypto FOMO, but Does It Belong i...   \n",
       "2  If you’re a Russian YouTuber, how do you get p...   \n",
       "3  El Salvador turns to Binance for help on bitco...   \n",
       "4                         Why Isn't Bitcoin Booming?   \n",
       "\n",
       "                                         description  \\\n",
       "0  Thomas Spieker’s clients included dark-web dru...   \n",
       "1  A growing array of investment options make it ...   \n",
       "2  Russian creators are shut off from the global ...   \n",
       "3  El Salvador is seeking support from cryptocurr...   \n",
       "4  \"Bitcoin was seen by many of its libertarian-l...   \n",
       "\n",
       "                                                text        date language  \n",
       "0  Mr. Spiekers most prominent customer, whom he ...  2022-03-24       en  \n",
       "1  Other fund vehicles hold crypto directly, but ...  2022-03-24       en  \n",
       "2  When Russia invaded Ukraine, Niki Proshin was ...  2022-03-17       en  \n",
       "3  SAN SALVADOR, March 23 (Reuters) - El Salvador...  2022-03-23       en  \n",
       "4  \"Bitcoin was seen by many of its libertarian-l...  2022-03-12       en  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame with Bitcoin news\n",
    "bitcoin_df = create_df(bitcoin_news_en[\"articles\"], \"en\")\n",
    "bitcoin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save bitcoin dataframe to CSV\n",
    "file_path = (\".\\dataframe_csv/bitcoin_news.csv\")\n",
    "bitcoin_df.to_csv(file_path, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ex-Party Producer Charged in $2.7 Million Bitc...</td>\n",
       "      <td>Thomas Spieker’s clients included dark-web dru...</td>\n",
       "      <td>Mr. Spiekers most prominent customer, whom he ...</td>\n",
       "      <td>2022-03-24</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Everyone Has Crypto FOMO, but Does It Belong i...</td>\n",
       "      <td>A growing array of investment options make it ...</td>\n",
       "      <td>Other fund vehicles hold crypto directly, but ...</td>\n",
       "      <td>2022-03-24</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you’re a Russian YouTuber, how do you get p...</td>\n",
       "      <td>Russian creators are shut off from the global ...</td>\n",
       "      <td>When Russia invaded Ukraine, Niki Proshin was ...</td>\n",
       "      <td>2022-03-17</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El Salvador turns to Binance for help on bitco...</td>\n",
       "      <td>El Salvador is seeking support from cryptocurr...</td>\n",
       "      <td>SAN SALVADOR, March 23 (Reuters) - El Salvador...</td>\n",
       "      <td>2022-03-23</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why Isn't Bitcoin Booming?</td>\n",
       "      <td>\"Bitcoin was seen by many of its libertarian-l...</td>\n",
       "      <td>\"Bitcoin was seen by many of its libertarian-l...</td>\n",
       "      <td>2022-03-12</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Ex-Party Producer Charged in $2.7 Million Bitc...   \n",
       "1  Everyone Has Crypto FOMO, but Does It Belong i...   \n",
       "2  If you’re a Russian YouTuber, how do you get p...   \n",
       "3  El Salvador turns to Binance for help on bitco...   \n",
       "4                         Why Isn't Bitcoin Booming?   \n",
       "\n",
       "                                         description  \\\n",
       "0  Thomas Spieker’s clients included dark-web dru...   \n",
       "1  A growing array of investment options make it ...   \n",
       "2  Russian creators are shut off from the global ...   \n",
       "3  El Salvador is seeking support from cryptocurr...   \n",
       "4  \"Bitcoin was seen by many of its libertarian-l...   \n",
       "\n",
       "                                                text        date language  \n",
       "0  Mr. Spiekers most prominent customer, whom he ...  2022-03-24       en  \n",
       "1  Other fund vehicles hold crypto directly, but ...  2022-03-24       en  \n",
       "2  When Russia invaded Ukraine, Niki Proshin was ...  2022-03-17       en  \n",
       "3  SAN SALVADOR, March 23 (Reuters) - El Salvador...  2022-03-23       en  \n",
       "4  \"Bitcoin was seen by many of its libertarian-l...  2022-03-12       en  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame with Ethereum news\n",
    "ethereum_df = create_df(ethereum_news_en[\"articles\"], \"en\")\n",
    "ethereum_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Ethereum dataframe to CSV\n",
    "file_path = (\".\\dataframe_csv/ethereum_news.csv\")\n",
    "ethereum_df.to_csv(file_path, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ex-Party Producer Charged in $2.7 Million Bitc...</td>\n",
       "      <td>Thomas Spieker’s clients included dark-web dru...</td>\n",
       "      <td>Mr. Spiekers most prominent customer, whom he ...</td>\n",
       "      <td>2022-03-24</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Everyone Has Crypto FOMO, but Does It Belong i...</td>\n",
       "      <td>A growing array of investment options make it ...</td>\n",
       "      <td>Other fund vehicles hold crypto directly, but ...</td>\n",
       "      <td>2022-03-24</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you’re a Russian YouTuber, how do you get p...</td>\n",
       "      <td>Russian creators are shut off from the global ...</td>\n",
       "      <td>When Russia invaded Ukraine, Niki Proshin was ...</td>\n",
       "      <td>2022-03-17</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El Salvador turns to Binance for help on bitco...</td>\n",
       "      <td>El Salvador is seeking support from cryptocurr...</td>\n",
       "      <td>SAN SALVADOR, March 23 (Reuters) - El Salvador...</td>\n",
       "      <td>2022-03-23</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why Isn't Bitcoin Booming?</td>\n",
       "      <td>\"Bitcoin was seen by many of its libertarian-l...</td>\n",
       "      <td>\"Bitcoin was seen by many of its libertarian-l...</td>\n",
       "      <td>2022-03-12</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Ex-Party Producer Charged in $2.7 Million Bitc...   \n",
       "1  Everyone Has Crypto FOMO, but Does It Belong i...   \n",
       "2  If you’re a Russian YouTuber, how do you get p...   \n",
       "3  El Salvador turns to Binance for help on bitco...   \n",
       "4                         Why Isn't Bitcoin Booming?   \n",
       "\n",
       "                                         description  \\\n",
       "0  Thomas Spieker’s clients included dark-web dru...   \n",
       "1  A growing array of investment options make it ...   \n",
       "2  Russian creators are shut off from the global ...   \n",
       "3  El Salvador is seeking support from cryptocurr...   \n",
       "4  \"Bitcoin was seen by many of its libertarian-l...   \n",
       "\n",
       "                                                text        date language  \n",
       "0  Mr. Spiekers most prominent customer, whom he ...  2022-03-24       en  \n",
       "1  Other fund vehicles hold crypto directly, but ...  2022-03-24       en  \n",
       "2  When Russia invaded Ukraine, Niki Proshin was ...  2022-03-17       en  \n",
       "3  SAN SALVADOR, March 23 (Reuters) - El Salvador...  2022-03-23       en  \n",
       "4  \"Bitcoin was seen by many of its libertarian-l...  2022-03-12       en  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate dataframes\n",
    "crypto_df = pd.concat([bitcoin_df, ethereum_df])\n",
    "crypto_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined dataframe to CSV\n",
    "file_path = (\"./dataframe_csv\\crypto_news.csv\")\n",
    "crypto_df.to_csv(file_path, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlating Returns\n",
    "#### Ethereum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETH (Ethereum)\n",
    "# Set the ticker\n",
    "ticker = \"ETH\"\n",
    "\n",
    "# Set timeframe to '1D'\n",
    "timeframe = \"1D\"\n",
    "\n",
    "# Set current date and the date from one month ago using the ISO format\n",
    "current_date = pd.Timestamp(datetime.now(), tz=\"America/New_York\").isoformat()\n",
    "past_date = pd.Timestamp(datetime.now()- timedelta(20), tz=\"America/New_York\").isoformat()\n",
    "\n",
    "# Get historical data for ETH\n",
    "eth_returns = api.get_barset(\n",
    "            ticker,\n",
    "            timeframe,\n",
    "            limit=None,\n",
    "            start=past_date,\n",
    "            end=current_date,\n",
    "            after=None,\n",
    "            until=None,\n",
    ").eth_returns\n",
    "\n",
    "# Display data\n",
    "eth_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Drop Outer Table Level\n",
    "eth_returns = eth_returns.droplevel(axis=1, level=0)\n",
    "\n",
    "# Use the drop function to drop extra columns\n",
    "eth_returns = eth_returns.drop(columns=[\"open\", \"high\", \"low\", \"volume\"])\n",
    "\n",
    "# Since this is daily data, we can keep only the date (remove the time) component of the data\n",
    "eth_returns.index = eth_returns.index.date\n",
    "\n",
    "# Display sample data\n",
    "eth_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Use the `pct_change` function to calculate daily returns of Bitcoin\n",
    "eth_returns = eth_returns.pct_change().dropna()\n",
    "eth_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JPEG\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bitcoin returns\n",
    "ticker = \"BTC\"\n",
    "\n",
    "# Get historical data for BTC\n",
    "btc_returns = api.get_barset(\n",
    "            ticker,\n",
    "            timeframe,\n",
    "            limit=None,\n",
    "            start=past_date,\n",
    "            end=current_date,\n",
    "            after=None,\n",
    "            until=None,\n",
    ").btc_returns\n",
    "\n",
    "# Display data\n",
    "btc_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Outer Table Level\n",
    "btc_returns = btc_returns.droplevel(axis=1, level=0)\n",
    "\n",
    "# Use the drop function to drop extra columns\n",
    "btc_returns = btc_returns.drop(columns=[\"open\", \"high\", \"low\", \"volume\"])\n",
    "\n",
    "# Since this is daily data, we can keep only the date (remove the time) component of the data\n",
    "btc_returns.index = btc_returns.index.date\n",
    "\n",
    "# Display sample data\n",
    "btc_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Use the `pct_change` function to calculate daily returns of Bitcoin\n",
    "btc_returns = btc_returns.pct_change().dropna()\n",
    "btc_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JPEG\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use newsapi client to get most relevant 20 headlines per day in the past month\n",
    "def get_headlines(keyword):\n",
    "    all_headlines = []\n",
    "    all_dates = []    \n",
    "    date = datetime.strptime(current_date[:10], \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(past_date[:10], \"%Y-%m-%d\")\n",
    "    print(f\"Fetching news about '{keyword}'\")\n",
    "    print(\"*\" * 30)\n",
    "    while date > end_date:\n",
    "        print(f\"retrieving news from: {date}\")\n",
    "        articles = newsapi.get_everything(\n",
    "            q=keyword,\n",
    "            from_param=str(date)[:10],\n",
    "            to=str(date)[:10],\n",
    "            language=\"en\",\n",
    "            sort_by=\"relevancy\",\n",
    "            page=1,\n",
    "        )\n",
    "        headlines = []\n",
    "        for i in range(0, len(articles[\"articles\"])):\n",
    "            headlines.append(articles[\"articles\"][i][\"title\"])\n",
    "        all_headlines.append(headlines)\n",
    "        all_dates.append(date)\n",
    "        date = date - timedelta(days=1)\n",
    "    return all_headlines, all_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve ethereum news headlines\n",
    "eth_headlines, dates = get_headlines(\"ethereum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve bitcoin news headlines\n",
    "btc_headlines, _ = get_headlines(\"bitcoin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that computes average compound sentiment of headlines for each day\n",
    "def headline_sentiment_summarizer_avg(headlines):\n",
    "    sentiment = []\n",
    "    for day in headlines:\n",
    "        day_score = []\n",
    "        for h in day:\n",
    "            if h == None:\n",
    "                continue\n",
    "            else:\n",
    "                day_score.append(sid.polarity_scores(h)[\"compound\"])\n",
    "        sentiment.append(sum(day_score) / len(day_score))\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Ethereum sentiment scores DataFrame\n",
    "def get_sentiment(score):\n",
    "    \"\"\"\n",
    "    Calculates the sentiment based on the compound score.\n",
    "    \"\"\"\n",
    "    result = 0  # Neutral by default\n",
    "    if score >= 0.05:  # Positive\n",
    "        result = 1\n",
    "    elif score <= -0.05:  # Negative\n",
    "        result = -1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get averages of each topics sentiment\n",
    "eth_avg = headline_sentiment_summarizer_avg(eth_headlines)\n",
    "btc_avg = headline_sentiment_summarizer_avg(btc_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Sentiment Averages into DataFrame\n",
    "topic_sentiments = pd.DataFrame(\n",
    "    {\n",
    "        \"eth_avg\": eth_avg,\n",
    "        \"btc_avg\": btc_avg,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index value of the sentiment averages DataFrame to be the series of dates.\n",
    "topic_sentiments.index = pd.to_datetime(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with BTC & ETH returns\n",
    "topic_sentiments = eth_returns.join(topic_sentiments).dropna(how=\"any\")\n",
    "\n",
    "# Display data\n",
    "display(topic_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlate the headlines' sentiment to returns\n",
    "topic_sentiments.corr().style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment scores dictionaries (Ethereum)\n",
    "title_sent = {\n",
    "    \"title_compound\": [],\n",
    "    \"title_pos\": [],\n",
    "    \"title_neu\": [],\n",
    "    \"title_neg\": [],\n",
    "    \"title_sent\": [],\n",
    "}\n",
    "text_sent = {\n",
    "    \"text_compound\": [],\n",
    "    \"text_pos\": [],\n",
    "    \"text_neu\": [],\n",
    "    \"text_neg\": [],\n",
    "    \"text_sent\": [],\n",
    "}\n",
    "\n",
    "# Get sentiment for the text and the title\n",
    "for index, row in ethereum_df.iterrows():\n",
    "    try:\n",
    "        # Sentiment scoring with VADER\n",
    "        title_sentiment = analyzer.polarity_scores(row[\"title\"])\n",
    "        title_sent[\"title_compound\"].append(title_sentiment[\"compound\"])\n",
    "        title_sent[\"title_pos\"].append(title_sentiment[\"pos\"])\n",
    "        title_sent[\"title_neu\"].append(title_sentiment[\"neu\"])\n",
    "        title_sent[\"title_neg\"].append(title_sentiment[\"neg\"])\n",
    "        title_sent[\"title_sent\"].append(get_sentiment(title_sentiment[\"compound\"]))\n",
    "\n",
    "        text_sentiment = analyzer.polarity_scores(row[\"text\"])\n",
    "        text_sent[\"text_compound\"].append(text_sentiment[\"compound\"])\n",
    "        text_sent[\"text_pos\"].append(text_sentiment[\"pos\"])\n",
    "        text_sent[\"text_neu\"].append(text_sentiment[\"neu\"])\n",
    "        text_sent[\"text_neg\"].append(text_sentiment[\"neg\"])\n",
    "        text_sent[\"text_sent\"].append(get_sentiment(text_sentiment[\"compound\"]))\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "# Attaching sentiment columns to the Ethereum news dataframe\n",
    "title_sentiment_df = pd.DataFrame(title_sent)\n",
    "text_sentiment_df = pd.DataFrame(text_sent)\n",
    "ethereum_df = ethereum_df.join(title_sentiment_df).join(text_sentiment_df)\n",
    "\n",
    "# Inspect new dataframe \n",
    "ethereum_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ethereum dataframe to CSV\n",
    "file_path = (\".\\dataframe_csv/eth_sentiment.csv\")\n",
    "ethereum_df.to_csv(file_path, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ETH sentiment dataframe as JPEG\n",
    "dfi.export(ethereum_df, './images\\ethereum_df.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment scores dictionaries (Bitcoin)\n",
    "title_sent = {\n",
    "    \"title_compound\": [],\n",
    "    \"title_pos\": [],\n",
    "    \"title_neu\": [],\n",
    "    \"title_neg\": [],\n",
    "    \"title_sent\": [],\n",
    "}\n",
    "text_sent = {\n",
    "    \"text_compound\": [],\n",
    "    \"text_pos\": [],\n",
    "    \"text_neu\": [],\n",
    "    \"text_neg\": [],\n",
    "    \"text_sent\": [],\n",
    "}\n",
    "\n",
    "# Get sentiment for the text and the title\n",
    "for index, row in bitcoin_df.iterrows():\n",
    "    try:\n",
    "        # Sentiment scoring with VADER\n",
    "        title_sentiment = analyzer.polarity_scores(row[\"title\"])\n",
    "        title_sent[\"title_compound\"].append(title_sentiment[\"compound\"])\n",
    "        title_sent[\"title_pos\"].append(title_sentiment[\"pos\"])\n",
    "        title_sent[\"title_neu\"].append(title_sentiment[\"neu\"])\n",
    "        title_sent[\"title_neg\"].append(title_sentiment[\"neg\"])\n",
    "        title_sent[\"title_sent\"].append(get_sentiment(title_sentiment[\"compound\"]))\n",
    "\n",
    "        text_sentiment = analyzer.polarity_scores(row[\"text\"])\n",
    "        text_sent[\"text_compound\"].append(text_sentiment[\"compound\"])\n",
    "        text_sent[\"text_pos\"].append(text_sentiment[\"pos\"])\n",
    "        text_sent[\"text_neu\"].append(text_sentiment[\"neu\"])\n",
    "        text_sent[\"text_neg\"].append(text_sentiment[\"neg\"])\n",
    "        text_sent[\"text_sent\"].append(get_sentiment(text_sentiment[\"compound\"]))\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "# Attaching sentiment columns to the Ethereum news dataframe\n",
    "title_sentiment_df = pd.DataFrame(title_sent)\n",
    "text_sentiment_df = pd.DataFrame(text_sent)\n",
    "bitcoin_df = bitcoin_df.join(title_sentiment_df).join(text_sentiment_df)\n",
    "\n",
    "# Inspect new dataframe \n",
    "bitcoin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save BTH sentiment dataframe as CSV\n",
    "file_path = (\".\\dataframe_csv/bitcoin_sentiment.csv\")\n",
    "bitcoin_df.to_csv(file_path, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save btc sentiment df as JPEG\n",
    "dfi.export(bitcoin_df, '.\\images/bitcoin_df.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the Bitcoin Sentiment\n",
    "bitcoin_df.plot(\n",
    "    y=[\"title_sent\", \"text_sent\"],\n",
    "    kind=\"bar\",\n",
    "    title=\"Bitcoin News Title and Text Sentiment Comparisson\",\n",
    "    figsize=(10, 8),\n",
    "    grid=True,\n",
    ")\n",
    "\n",
    "# Save plot as JPEG\n",
    "plt.savefig(\".\\images/bitcoin_sentiment.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe bitcoin dataframe\n",
    "bitcoin_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the Ethereum Sentiment\n",
    "ethereum_df.plot(\n",
    "    y=[\"title_sent\", \"text_sent\"],\n",
    "    kind=\"bar\",\n",
    "    title=\"Ethereum News Title and Text Sentiment Comparisson\",\n",
    "    figsize=(10, 8),\n",
    "    grid=True,\n",
    ")\n",
    "\n",
    "# Save plot as JPEG\n",
    "plt.savefig(\".\\images/eth_sentiment.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the Ethereum dataframe\n",
    "ethereum_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "Q: Which coin had the highest mean positive score?\n",
    "\n",
    "A: \n",
    "\n",
    "Q: Which coin had the highest compound score?\n",
    "\n",
    "A: \n",
    "\n",
    "Q. Which coin had the highest positive score?\n",
    "\n",
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Natural Language Processing\n",
    "---\n",
    "###   Tokenizer\n",
    "\n",
    "In this section, you will use NLTK and Python to tokenize the text for each coin. Be sure to:\n",
    "1. Lowercase each word.\n",
    "2. Remove Punctuation.\n",
    "3. Remove Stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from string import punctuation\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the lemmatizer (Bitcoin)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "article = bitcoin_df\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the lemmatizer (Bitcoin)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "article = ethereum_df\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of stopwords\n",
    "sw = set(stopwords.words('english'))\n",
    "\n",
    "# Expand the default stopwords list if necessary\n",
    "sw_addons = { \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \n",
    "              \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \n",
    "              \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \n",
    "              \"anyone\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appreciate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"arise\", \"around\", \"as\", \"aside\", \"ask\", \n",
    "              \"asking\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"B\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"been\", \"before\", \n",
    "              \"beforehand\", \"beginnings\", \"behind\", \"below\", \"beside\", \"besides\", \"best\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \n",
    "              \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"C\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \n",
    "              \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"could\", \"couldn\", \"couldnt\", \"course\", \n",
    "              \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"D\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \n",
    "              \"di\", \"did\", \"didn\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doing\", \"don\", \"done\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"E\", \"e2\", \n",
    "              \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \n",
    "              \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \n",
    "              \"example\", \"except\", \"ey\", \"f\", \"F\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \n",
    "              \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"G\", \"ga\", \n",
    "              \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"H\", \n",
    "              \"h2\", \"h3\", \"had\", \"hadn\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"have\", \"haven\", \"having\", \"he\", \"hed\", \"hello\", \"help\", \"hence\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \n",
    "              \"hereupon\", \"hes\", \"hh\", \"hi\", \"hid\", \"hither\", \"hj\", \"ho\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \n",
    "              \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"im\", \"immediately\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \n",
    "              \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"it\", \"itd\", \"its\", \"iv\", \"ix\", \"iy\", \"iz\", \"j\", \"J\", \n",
    "              \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"K\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"ko\", \"l\", \"L\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \n",
    "              \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \n",
    "              \"ls\", \"lt\", \"ltd\", \"m\", \"M\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mill\", \"million\", \n",
    "              \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"my\", \"n\", \"N\", \"n2\", \"na\", \"name\", \n",
    "              \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"neither\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \n",
    "              \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"O\", \"oa\", \"ob\", \"obtain\", \"obtained\", \n",
    "              \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \n",
    "              \"or\", \"ord\", \"os\", \"ot\", \"otherwise\", \"ou\", \"ought\", \"our\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"P\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \n",
    "              \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \n",
    "              \"po\", \"poorly\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"Q\", \"qj\", \n",
    "              \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"R\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \n",
    "              \"regardless\", \"regards\", \"related\", \"relatively\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \n",
    "              \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"S\", \"s2\", \"sa\", \"said\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"seem\", \n",
    "              \"seemed\", \"seeming\", \"seems\", \"seen\", \"sent\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"since\", \"sincere\", \n",
    "              \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somehow\", \"somethan\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \n",
    "              \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \n",
    "              \"sy\", \"sz\", \"t\", \"T\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"thats\", \n",
    "              \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"thereof\", \"therere\", \"theres\", \"thereto\", \n",
    "              \"thereupon\", \"these\", \"they\", \"theyd\", \"theyre\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \n",
    "              \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \n",
    "              \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"U\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \n",
    "              \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"using\", \"usually\", \"ut\", \"v\", \n",
    "              \"V\", \"va\", \"various\", \"vd\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"W\", \"wa\", \"was\", \"wasn\", \"wasnt\", \"way\", \"we\", \"wed\", \n",
    "              \"welcome\", \"well\", \"well-b\", \"went\", \"were\", \"weren\", \"werent\", \"what\", \"whatever\", \"whats\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \n",
    "              \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"whom\", \"whomever\", \"whos\", \"whose\", \"why\", \"wi\", \"widely\", \n",
    "              \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"would\", \"wouldn\", \"wouldnt\", \"www\", \"x\", \"X\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \n",
    "              \"xv\", \"xx\", \"y\", \"Y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"your\", \"youre\", \"yours\", \"yr\", \"ys\", \"yt\", \"z\", \"Z\", \"zero\", \"zi\", \"zz\"}\n",
    "stop = set(list(sw)+list(sw_addons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def tokenizer(text):\n",
    "    sw = set(stopwords.words('english'))\n",
    "    regex = re.compile(\"[^a-zA-Z ]\")\n",
    "    re_clean = regex.sub('', article)\n",
    "    words = word_tokenize(re_clean)\n",
    "    lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "    output = [word.lower() for word in lem if word.lower() not in sw]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the tokenizer function\n",
    "def tokenizer(text):\n",
    "    \"\"\"Tokenizes text.\"\"\"\n",
    "    ## Define and apply the regex parameters\n",
    "    regex = re.compile(\"[^a-zA-Z ]\")\n",
    "    re_clean = regex.sub('', text)\n",
    "    \n",
    "    # Create a tokenized list of the words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Convert the words to lowercase\n",
    "    words = list(filter(lambda w: w.lower(), words))\n",
    "    \n",
    "    # Remove the punctuation from text\n",
    "    words = list(filter(lambda t: t not in punctuation, words))\n",
    "    \n",
    "    # Remove the stop words\n",
    "    words = list(filter(lambda t: t.lower() not in stop, words))\n",
    "    \n",
    "    # Lemmatize words into root words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tokens column for Bitcoin\n",
    "bitcoin_df[\"tokens\"] = bitcoin_df.text.apply(tokenizer)\n",
    "bitcoin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to CSV\n",
    "file_path = (\".\\dataframe_csv/btc_with_token_column.csv\")\n",
    "bitcoin_df.to_csv(file_path, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tokens column for Ethereum\n",
    "ethereum_df[\"tokens\"] = ethereum_df.text.apply(tokenizer)\n",
    "ethereum_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to CSV\n",
    "file_path = (\".\\dataframe_csv/eth_with_token_column.csv\")\n",
    "ethereum_df.to_csv(file_path, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NGrams and Frequency Analysis\n",
    "\n",
    "In this section you will look at the ngrams and word frequency for each coin. \n",
    "\n",
    "1. Use NLTK to produce the n-grams for N = 2. \n",
    "2. List the top 10 words for each coin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the Bitcoin N-grams where N=2\n",
    "N = 2\n",
    "grams = ngrams(tokenizer(bitcoin_df.text.str.cat()), N)\n",
    "Counter(grams).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the Ethereum N-grams where N=2\n",
    "N = 2\n",
    "grams = ngrams(tokenizer(ethereum_df.text.str.cat()), N)\n",
    "Counter(grams).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function token_count generates the top 10 words for a given coin\n",
    "def token_count(tokens, N=3):\n",
    "    \"\"\"Returns the top N tokens from the frequency count\"\"\"\n",
    "    return Counter(tokens).most_common(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use token_count to get the top 10 words for Bitcoin\n",
    "btc_tokens = tokenizer(bitcoin_df.text.str.cat())\n",
    "token_count(btc_tokens, N = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use token_count to get the top 10 words for Ethereum\n",
    "eth_tokens = tokenizer(ethereum_df.text.str.cat())\n",
    "token_count(eth_tokens, N = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Clouds\n",
    "\n",
    "In this section, you will generate word clouds for each coin to summarize the news for each coin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = [20.0, 10.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the Bitcoin word cloud\n",
    "word_cloud = WordCloud().generate(' '.join(btc_tokens))\n",
    "plt.imshow(word_cloud)\n",
    "plt.title(\"Bitcoin Word Cloud\", fontdict={'fontsize':25})\n",
    "\n",
    "# Save as jpeg\n",
    "plt.savefig(\".\\images/btc_cloud.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the Ethereum word cloud\n",
    "word_cloud = WordCloud().generate(' '.join(eth_tokens))\n",
    "plt.imshow(word_cloud)\n",
    "plt.title(\"Ethereum Word Cloud\", fontdict={'fontsize':25})\n",
    "\n",
    "# Save as jpeg\n",
    "plt.savefig(\".\\images/eth_cloud.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Named Entity Recognition\n",
    "\n",
    "In this section, you will build a named entity recognition model for both Bitcoin and Ethereum, then visualize the tags using SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Bitcoin NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all of the Bitcoin text together\n",
    "bitcoin_NER = bitcoin_df[\"text\"].str.cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the NER processor on all of the text\n",
    "doc = nlp(bitcoin_NER)\n",
    "\n",
    "# Add a title to the document\n",
    "doc.user_data[\"title\"]=\"Bitcoin NER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the visualization\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all Entities\n",
    "for i in doc.ents:\n",
    "    print(i.text, i.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ethereum NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all of the Ethereum text together\n",
    "ethereum_NER = ethereum_df[\"text\"].str.cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the NER processor on all of the text\n",
    "doc = nlp(ethereum_NER)\n",
    "\n",
    "# Add a title to the document\n",
    "doc.user_data[\"title\"]=\"Ethereum NER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the visualization\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all Entities\n",
    "for i in doc.ents:\n",
    "    print(i.text, i.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
